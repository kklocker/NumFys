\section{Step-by-step time evolution}

For a time-dependent Hamiltonian, an expansion in stationary eigenstates does not work in the same simple fashion. This is because the states $\psi_i$ are eigenvectors of the \textit{instantaneous} Hamiltonian. For different times , these vectors are not any more eigenvectors of the full Hamiltonian. 
That being said, with a step-by-step implementation we should get rid of the apparent degeneracy of the eigenvalues. 

\subsection{Euler scheme}
We now implement the Euler scheme for evaluating its applicability for the quantum problem. An interesting phenomena occur, where the function abruptly breaks down after a number of time steps. The approach to finding the number of steps before this breakdown is by looking at the normalization of the wave function. As this suddenly diverges, we can use this as a check in a while-loop for comparing step sizes in both temporal and spatial direction. 
\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{img/cfl.png}
	\caption{The number of steps taken before the Wave Function breaks down against the Courant-Friedrichs-Lewy (CFL)-number on a log-log scale.}
	\label{fig:cfl}
\end{figure}

In \cref{fig:cfl} the steps before the breakdown of the simulation for the Euler-Scheme is shown. This suggests that for the a successful simulation in this scheme, we need $\Delta t' \ll (\Delta x')^2$. At thee same time we also want $\Delta x' \ll 1$, so this would require a very heavy computation. A better approach is to use a different numerical scheme. 

 
\subsection{Crank-Nicolson}

The implementation of this scheme is done by computing the LU-decomposition of $\left(1+\frac{i}{2}\Delta t'\hat H\right)$ and solving a linear system $A\vb x= b$ for each time step. 
As previously mentioned, we should get rid of the degeneracy of states. With this in mind, the phase  $\sim \frac{1}{\lambda_2-\lambda_1}$ should diverge, and we have to propagate the system infinitely in time.  By preparing the same state as in \cref{eq:psi0}, we can use the Crank-Nicolson scheme to propagate the function and check this. 
\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{img/crank.png}
	\caption{Initial state and time-evolved state using the Crank-Nicolson scheme. }
	\label{fig:crank}
\end{figure}
As viewed in \cref{fig:crank}, there is now no tunneling, as opposed to the case when we project the states on a plane-wave basis, c.f. \cref{fig:tunneling}.

\subsection{Two level system}

\cref{fig:e1e2} shows the two lowest lying energy eigenvalues for $V(x, t)$ as introduced in  ref.\cite{assignment}. For $\nu_r = 0$, the energy difference is $\varepsilon_0 \simeq 5.6962$. 
\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{img/e1_e2.png}
	\caption{The two lowest eigenvalues plotted against the varying potential $\nu_r$. }
	\label{fig:e1e2}
\end{figure}

The expectation value 
\begin{equation} 
\tau = \mel{g_0}{\hat H}{e_0}
\end{equation}
can be computed as the previous inner products, applying first the action of $\hat H$ to $\ket{e_0}$ and then taking the inner product. $\tau$ is found to take a linear shape 
\begin{equation}
	\tau(\nu_r) \simeq -0.429\nu_r.
\end{equation}

Let us next discretize the Volterra integral equation
\begin{equation} 
\ket{\psi(t=k\Delta t)}_I = \ket{\psi(0)} -\frac{i}{\hbar}\int_0^t\dd{t'}H_{1I}(t')\ket{\psi(t')}_I
\end{equation}
using the Trapezoidal rule
\begin{equation} 
\int_0^tf(t)\dd{t} \simeq \sum_{k=1}^{N}\frac{f((k-1)\Delta t)+f(k\Delta t)}{2}\Delta t.
\end{equation}
We get
\begin{align*} 
\ket{0} = &\ket{0} \\
\ket{\Delta t} = &\ket{0} - \frac{i}{\hbar}\Delta t H(0)\ket{0} \\
\ket{2\Delta t} = &\ket{0} - \frac{i}{\hbar}\Delta t \left(H(0)\ket{0} + H(\Delta t)\ket{\Delta t}\right) \\
&\vdots \\
\ket{n\Delta t} = &\ket{0}-\frac{i}{\hbar}\Delta t\left(\sum_{k=1}^{n-2}H(k\Delta t)\right. \\ 
&\qquad+ \left.\frac{H(0)\ket{0} + H((n-1)\Delta t)\ket{(n-1)\Delta t}}{2}\vphantom{(\sum_{k=1}^{n-2}}\right).
\end{align*}
The implementation of this discretization could be done by clever matrix manipulations. It could also be done with a simpler implementation, looping over all states previously calculated. This is a more ``brute-force'' way of doing it, but works.
\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{img/transition_prob.png}
	\caption{The probability of finding the system in the first excited state, $\ket{e_0}$.}
	\label{fig:trans}
\end{figure}
In \cref{fig:trans} a comparison of the implementation and an approximate solution for the probability of finding $\ket{e_0}$ given that the system was initially in $\ket{\Psi(t=0)} = \ket{g_0}$. This was done for $\omega = \varepsilon_0$ and $\tau = 0.02\varepsilon_0$. Notice the decoherence that occur in the computed probability, not present in the analytic expression\cite{assignment}
\begin{equation} 
p(t) = \sin[2](\frac{t\tau}{2\hbar}).
\end{equation}
If we now vary $\omega$ and $\tau$, we may notice the following. Setting $\omega \ne \varepsilon_0$ will lower the probability of finding the system in the state $\ket{e_0}$, and shifting the peaks. Increasing $\tau$ does not seem to alter the amplitude of the probability, but shifts the peaks to earlier times. This is visualized in \cref{fig:probs}, where secondary oscillations are more pronounced than in \cref{fig:trans}. 
\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{img/stuff.pdf}
	\caption{The transition probability $|\braket{g_0}{\psi(t)}|^2$ of three different configurations. }
	\label{fig:probs}
\end{figure}
If we choose $\tau$ too large, the method breaks down. This is clear from an interpretation of $\tau$ as a perturbation smallness parameter, in which a perturbation series would diverge for  $\tau>1$. 

